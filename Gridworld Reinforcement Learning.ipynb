{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic class for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Agent:\n",
    "    Object with a state, the history of past state, and total accumulated reward\n",
    "    \"\"\"\n",
    "    def __init__(self, state=None):\n",
    "        self._state = state\n",
    "        self._state_history = [state] if state != None else []\n",
    "        self._total_reward = 0\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "    \n",
    "    @state.setter\n",
    "    def state(self, new_state):\n",
    "        self._state = new_state\n",
    "        self._state_history.append(new_state)\n",
    "    \n",
    "    def reset(self, state=None):\n",
    "        self._state = state\n",
    "        self.resetHistory()\n",
    "        \n",
    "    def resetHistory(self):\n",
    "        self._state_history = [];\n",
    "    \n",
    "    def summary(self):\n",
    "        print('Current state : {}'.format(self._state))\n",
    "        print('State history : {}'.format(self._state_history))\n",
    "        print('Total reward  : {}'.format(self._total_reward))\n",
    "        \n",
    "    def move(self, next_state, reward):\n",
    "        self._state = next_state\n",
    "        self._state_history.append(next_state)\n",
    "        self._total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic class for environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An environment has : \n",
    "1. a list of state \n",
    "2. a action function that maps each state to the set of possible next states\n",
    "3. a reward function that maps each (action,state) to a reward\n",
    "4. a terminal state\n",
    "\n",
    "Those are defined here as abstract method to be specified in each environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(abc.ABC):\n",
    "    \"\"\"Environment:\n",
    "    Contains the state space, the (state,action)->new_state mapping, (state,action,new_state)->reward mapping\n",
    "    One of the states is the terminal state}\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._state_space = []\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def actionStateMap(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def getReward(self, state=None, action=None, next_state=None):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def isTerminal(self, state=None):\n",
    "        pass\n",
    "    \n",
    "    def getNexStateFromAction(self, state=None, action=None):\n",
    "        next_state = self.actionStateMap(state).get(action)\n",
    "        reward = self.getReward(state, action, next_state)\n",
    "        return next_state, reward\n",
    "    \n",
    "    def summary(self):\n",
    "        print('Terminal state: {}'.format(self._terminal_state))\n",
    "        for state in self._state_space:\n",
    "            print('{}'.format(state))\n",
    "            actions = self.actionStateMap(state)\n",
    "            for action, next_state in actions.items():\n",
    "                print('\\t{} --> {}'.format(action, next_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridworld is a generic environment where the state space is chessboard-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld(Environment):\n",
    "    \"\"\"Chessboard-like environment\n",
    "    \"\"\"\n",
    "    def __init__(self, n_h, n_v, terminal_state=None):\n",
    "        super().__init__()\n",
    "        self._n_h = n_h # number of horizontal cells\n",
    "        self._n_v = n_v # number of vertical cells\n",
    "        self._terminal_state = terminal_state\n",
    "        self._directions = {\n",
    "            'right': [1, 0],\n",
    "            'left': [-1, 0],\n",
    "            'up': [0, 1],\n",
    "            'down': [0, -1],\n",
    "        }\n",
    "        # states are tuples (i,j) representing cells of the grid\n",
    "        self._state_space = [(i,j) for i in range(n_h) for j in range(n_v)]\n",
    "            \n",
    "    def resetAgentState(self, Agent, state=None):\n",
    "        # random reset, except if a state is specified\n",
    "        if state != None:\n",
    "            Agent.reset(state=state)\n",
    "        else:\n",
    "            i = rd.choice(range(len(self._state_space)))\n",
    "            Agent.reset(state=self._state_space[i])\n",
    "    \n",
    "    def isTerminal(self, state=None):\n",
    "        return state == self._terminal_state\n",
    "        \n",
    "    def isLeftWall(self, state):\n",
    "        return state[0] <= 0\n",
    "    \n",
    "    def isRightWall(self, state):\n",
    "        return state[0] >= self._n_h-1\n",
    "    \n",
    "    def isDownWall(self, state):\n",
    "        return state[1] <= 0\n",
    "\n",
    "    def isUpWall(self, state):\n",
    "        return state[1] >= self._n_v-1\n",
    "    \n",
    "    def printPath(self, state_history, reward_history):\n",
    "        grid = np.full((self._n_h, self._n_v), np.nan)\n",
    "        for state, reward in zip(list(state_history),list(reward_history)):\n",
    "            grid[state] = reward\n",
    "                                 \n",
    "        np.set_printoptions(nanstr='.')\n",
    "        # need to flip the h-axis since grid is a matrix\n",
    "        print(np.flip(grid.T,0))\n",
    "        print('Total reward : {}'.format(sum(reward_history)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BasicGridworld, you can access all available right/left/up/down squares, with a reward of -1 for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGridworld(Gridworld):\n",
    "    \"\"\"Simple grid, only up/down/right/left actions\n",
    "    \"\"\"\n",
    "    def __init__(self, n_h, n_v, terminal_state=None):\n",
    "        super().__init__(n_h, n_v, terminal_state)\n",
    "        \n",
    "    def actionStateMap(self, state):\n",
    "        directions = self._directions.copy()\n",
    "         \n",
    "        if self.isLeftWall(state):\n",
    "            del directions['left']\n",
    "        \n",
    "        if self.isRightWall(state):\n",
    "            del directions['right']\n",
    "        \n",
    "        if self.isUpWall(state):\n",
    "            del directions['up']\n",
    "        \n",
    "        if self.isDownWall(state):\n",
    "            del directions['down']\n",
    "        \n",
    "        # lambda to wrap the transition from state using up/down/left/right\n",
    "        move_lambda = lambda direction: tuple(np.array(state)+np.array(direction))\n",
    "        \n",
    "        # returns list of possible actions, list of possible next_states\n",
    "        return dict(zip(list(directions.keys()), list(map(move_lambda, list(directions.values())))))\n",
    "\n",
    "    def getReward(self, state=None, action=None, next_state=None):\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In WindyGridworld, there's an upward wind blowing through the middle of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridworld(Gridworld):\n",
    "    \"\"\"Simple grid, the vertical median line has a wind effect that adds an extra up:\n",
    "    \"\"\"\n",
    "    def __init__(self, n_h, n_v, terminal_state=None):\n",
    "        super().__init__(n_h, n_v, terminal_state)\n",
    "        \n",
    "    def actionStateMap(self, state):\n",
    "        directions = self._directions.copy()\n",
    "        \n",
    "        if self.isLeftWall(state):\n",
    "            del directions['left']\n",
    "        \n",
    "        if self.isRightWall(state):\n",
    "            del directions['right']\n",
    "        \n",
    "        if self.isUpWall(state):\n",
    "            del directions['up']\n",
    "        \n",
    "        if self.isDownWall(state):\n",
    "            del directions['down']\n",
    "        \n",
    "        # if state is on the vertical median line of the grid, each move is affected by an extra up (if possible)\n",
    "        if state[0] == int(self._n_h/2):\n",
    "            for key, direction in directions.items():\n",
    "                potential_next_state = tuple(np.array(state)+np.array(direction))\n",
    "                if not self.isUpWall(potential_next_state):\n",
    "                    directions[key] = tuple(np.array(direction)+np.array([0, 1])) \n",
    "            \n",
    "        # lambda to wrap the transition from state using up/down/left/right\n",
    "        move_lambda = lambda direction: tuple(np.array(state)+np.array(direction))\n",
    "        \n",
    "        # returns list of possible actions, list of possible next_states\n",
    "        return dict(zip(list(directions.keys()), list(map(move_lambda, list(directions.values())))))\n",
    "        \n",
    "    def getReward(self, state=None, action=None, next_state=None):\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in CliffGridWorld, there is a cliff at the bottom of the grid : fall and you start over with a massive negative reward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffGridworld(Gridworld):\n",
    "    \"\"\"Simple grid, the bottom row of is a cliff that penalizes the agent if it falls over it\n",
    "    \"\"\"\n",
    "    def __init__(self, n_h, n_v, startover_state=None, terminal_state=None):\n",
    "        super().__init__(n_h, n_v, terminal_state)\n",
    "        # initial_state is where the agent is brought back if it falls in the cliff\n",
    "        self._startover_state = startover_state\n",
    "        \n",
    "    def printCliff(self):\n",
    "        grid = np.full((self._n_h, self._n_v), '.')\n",
    "        for i in range(1,self._n_h-1):\n",
    "            grid[(i, 0)] = '*'\n",
    "        # need to flip the h-axis since grid is a matrix\n",
    "        print(np.flip(grid.T,0))\n",
    "        \n",
    "    # overload summary to add a print of the cliff\n",
    "    def summary(self):\n",
    "        self.printCliff()\n",
    "        super().summary()\n",
    "        \n",
    "    # if right above the cliff and the action is 'down' or if at the bottom-left corner and the action is 'right'\n",
    "    def fallsInHole(self, state, action):\n",
    "        return ((state[1] == 1 and state[0] not in [0, self._n_h-1]) and action == 'down') or (state == (0, 0) and action == 'right')\n",
    "            \n",
    "    def actionStateMap(self, state):\n",
    "        directions = self._directions.copy()\n",
    "        \n",
    "        if self.isLeftWall(state):\n",
    "            del directions['left']\n",
    "        \n",
    "        if self.isRightWall(state):\n",
    "            del directions['right']\n",
    "        \n",
    "        if self.isUpWall(state):\n",
    "            del directions['up']\n",
    "        \n",
    "        if self.isDownWall(state):\n",
    "            del directions['down']\n",
    "           \n",
    "        for key, direction in directions.items():\n",
    "            if self.fallsInHole(state, key):\n",
    "                directions[key] = tuple(np.array(self._startover_state)-np.array(state))\n",
    "                \n",
    "        # lambda to wrap the transition from state using up/down/left/right\n",
    "        move_lambda = lambda direction: tuple(np.array(state)+np.array(direction))\n",
    "        \n",
    "        # returns list of possible actions, list of possible next_states\n",
    "        return dict(zip(list(directions.keys()), list(map(move_lambda, list(directions.values())))))\n",
    "        \n",
    "    def getReward(self, state=None, action=None, next_state=None):\n",
    "        if self.fallsInHole(state, action):\n",
    "            return -100\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyPickAction(state, Q):\n",
    "    \"\"\"Greedy policy:\n",
    "    for a given state, returns the action that maximizes the current estimate of the Q function\n",
    "    \"\"\"\n",
    "    return max(Q[state].items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \"\"\"Policy:\n",
    "    Contains a state->action mapping\n",
    "    \"\"\"\n",
    "    def __init__(self, Environment):\n",
    "        self._Environment = Environment\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def pickAction(self, state=None, context={}):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(Policy):\n",
    "    \"\"\"Epsilon Greedy policy:\n",
    "    for a given state, returns the action that maximizes the current estimate of the Q function\n",
    "    with probability 1-epsilion, otherwise randomly draw an action\n",
    "    \"\"\"\n",
    "    def __init__(self, Environment, epsilon):\n",
    "        super().__init__(Environment)\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "    def pickAction(self, state=None, context={}):\n",
    "        if rd.uniform() <= 1-self._epsilon: \n",
    "            return greedyPickAction(state, context.get('Q'))\n",
    "        else:\n",
    "            return rd.choice(list(self._Environment.actionStateMap(state).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(EpsilonGreedyPolicy):\n",
    "    \"\"\"Random policy:\n",
    "    for a given state, returns an action drawn uniformly at random\n",
    "    \"\"\"\n",
    "    def __init__(self, Environment):\n",
    "        super().__init__(Environment, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(EpsilonGreedyPolicy):\n",
    "    \"\"\"Greedy policy:\n",
    "    for a given state, returns the action that maximizes the current estimate of the Q function\n",
    "    \"\"\"\n",
    "    def __init__(self, Environment):\n",
    "        super().__init__(Environment, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueRL(abc.ABC):\n",
    "    \"\"\"ActionValueRL:\n",
    "        Performs policy optimization using action-value learning\n",
    "    \"\"\"\n",
    "    def __init__(self, Agent=None, Environment=None,params={}):\n",
    "        self._Agent = Agent\n",
    "        self._Environment = Environment\n",
    "        self._state_space = Environment._state_space\n",
    "        \n",
    "        # Q is a dict with keys {state} and values dict {action: value}\n",
    "        # To call Q(s,a): self._Q[state][action] \n",
    "        self._Q = dict(zip(self._Environment._state_space, [{},]*len(self._Environment._state_space)))\n",
    "        self._max_count = int(1e5) # max number of steps per epoch\n",
    "        self._iter_per_epoch = []\n",
    "        \n",
    "        self._policy_params = params.get('policy_params')\n",
    "        self._agent_params = params.get('agent_params')\n",
    "\n",
    "        # set Policy\n",
    "        self._policy_name = ''\n",
    "        self._Policy = None\n",
    "        self.reset(policy_params, agent_params)\n",
    "        \n",
    "    # initialize to Q(s,a)=0\n",
    "    def reset(self, policy_params={}, agent_params={}):\n",
    "        for state in self._state_space:\n",
    "            actions = self._Environment.actionStateMap(state)\n",
    "            q_state = {}\n",
    "            for action in actions:\n",
    "                q_state[action] = 0\n",
    "            self._Q[state] = q_state\n",
    "                \n",
    "        # if policy_params is not empty, reset policy as well\n",
    "        if policy_params:\n",
    "            self.setPolicy(policy_params)\n",
    "        \n",
    "        # if agent_params is not empty, reset agent as well\n",
    "        if agent_params:\n",
    "            self._Agent.reset(agent_params.get('state'))\n",
    "    \n",
    "    def getQ(self, state, action):\n",
    "        return self._Q[state][action]\n",
    "    \n",
    "    def setQ(self, state, action, new_value):\n",
    "        self._Q[state][action] = new_value\n",
    "        \n",
    "    def setPolicy(self, policy_params={}):\n",
    "        self._policy_name = policy_params.get('policy_name')\n",
    "        if self._policy_name == 'Random':\n",
    "            self._Policy = RandomPolicy(self._Environment)\n",
    "        elif self._policy_name == 'Greedy':\n",
    "            self._Policy = GreedyPolicy(self._Environment)\n",
    "        elif self._policy_name == 'EpsilonGreedy':\n",
    "            self._Policy = EpsilonGreedyPolicy(self._Environment, policy_params.get('epsilon'))\n",
    "        else:\n",
    "            raise NameError( 'Policy name not found : {}'.format(self._policy_name) )\n",
    "    \n",
    "    # optimal path is the one where each action is selected using the greedy policy\n",
    "    def optimalPath(self, initial_state):\n",
    "        count, total_reward = 0, 0\n",
    "        state = initial_state\n",
    "        state_history = [initial_state]\n",
    "        reward_history = [0]\n",
    "        Greedy = GreedyPolicy(self._Environment)\n",
    "        while not self._Environment.isTerminal(state) and count < self._max_count:\n",
    "            #action = max(self._Q[state].items(), key=operator.itemgetter(1))[0]\n",
    "            action = greedyPickAction(state, self._Q)\n",
    "            state, reward = self._Environment.getNexStateFromAction(state, action)\n",
    "            state_history.append(state)\n",
    "            count += 1\n",
    "            reward_history.append(reward)\n",
    "            \n",
    "        return state_history, reward_history\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def learn(self, n_epochs):\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa(ActionValueRL):\n",
    "    \"\"\"SARSA:\n",
    "        SARSA implementation of on-policy TD\n",
    "    \"\"\"\n",
    "    def __init__(self, Agent=None, Environment=None, params={}):\n",
    "        super().__init__(Agent=Agent, Environment=Environment, params=params)\n",
    "        self._learning_rate = params.get('learning_rate')\n",
    "        self._discount_factor = params.get('discount_factor')\n",
    "        \n",
    "    def learn(self, n_epochs, init_state):\n",
    "        for count_epoch in range(n_epochs):\n",
    "            # initialize state\n",
    "            self._Environment.resetAgentState(self._Agent, state=init_state)\n",
    "            state = self._Agent.state\n",
    "            \n",
    "            # choose action from state using policy derived from Q\n",
    "            action = self._Policy.pickAction(state, {'Q': self._Q})\n",
    "            count = 0\n",
    "            \n",
    "            while not self._Environment.isTerminal(state) and count < self._max_count:\n",
    "                # take action, observe next_state and reward\n",
    "                next_state, reward = self._Environment.getNexStateFromAction(state, action)\n",
    "                \n",
    "                # choose next_action from next_state using policy derived from Q\n",
    "                next_action = self._Policy.pickAction(next_state, {'Q': self._Q})\n",
    "                                \n",
    "                # compute TD target and update Q\n",
    "                TD_target = reward+self._discount_factor*self.getQ(next_state, next_action)-self.getQ(state, action)\n",
    "                new_value = self.getQ(state, action)+self._learning_rate*TD_target\n",
    "                self.setQ(state, action, new_value)\n",
    "                \n",
    "                # move to next state, update Agent internal state\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                self._Agent.move(next_state, reward)\n",
    "                count += 1\n",
    "            self._iter_per_epoch.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(ActionValueRL):\n",
    "    \"\"\"Q-learning:\n",
    "        Q-learning implementation of off-policy TD -- learn the optimal policy directly while following a different exploration policy\n",
    "    \"\"\"\n",
    "    def __init__(self, Agent=None, Environment=None, params={}):\n",
    "        super().__init__(Agent=Agent, Environment=Environment, params=params)\n",
    "        self._learning_rate = params.get('learning_rate')\n",
    "        self._discount_factor = params.get('discount_factor')\n",
    "        \n",
    "    def learn(self, n_epochs, init_state):\n",
    "        for count_epoch in range(n_epochs):\n",
    "            # initialize state\n",
    "            self._Environment.resetAgentState(self._Agent, state=init_state)\n",
    "            state = self._Agent.state\n",
    "            \n",
    "            count = 0\n",
    "            \n",
    "            while not self._Environment.isTerminal(state) and count < self._max_count:\n",
    "                 # choose action from state using policy derived from Q\n",
    "                action = self._Policy.pickAction(state, {'Q': self._Q})\n",
    "            \n",
    "                # take action, observe next_state and reward\n",
    "                next_state, reward = self._Environment.getNexStateFromAction(state, action)\n",
    "                                                \n",
    "                # off-policy learning : take the max of the Q function at the current state, regardless of the action followed by the agent\n",
    "                Q_max = max(self._Q[next_state].items(), key=operator.itemgetter(1))[1]\n",
    "                \n",
    "                # compute TD target and update Q\n",
    "                TD_target = reward+self._discount_factor*Q_max-self.getQ(state, action)\n",
    "                new_value = self.getQ(state, action)+self._learning_rate*TD_target\n",
    "                self.setQ(state, action, new_value)\n",
    "                \n",
    "                # move to next state, update Agent internal state\n",
    "                state = next_state\n",
    "                self._Agent.move(next_state, reward)\n",
    "                count += 1\n",
    "            self._iter_per_epoch.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Bob, the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state : (0, 0)\n",
      "State history : [(0, 0)]\n",
      "Total reward  : 0\n"
     ]
    }
   ],
   "source": [
    "Bob = Agent()\n",
    "init_state = (0,0)\n",
    "Bob.state = init_state\n",
    "Bob.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h, n_v = 10, 10\n",
    "terminal_state = (n_h-1, n_v-1)\n",
    "Env = BasicGridworld(n_h, n_v, terminal_state=terminal_state)\n",
    "#Env.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_params = {\n",
    "    'policy_name': 'EpsilonGreedy',\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    'state': init_state\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'policy_params': policy_params,\n",
    "    'agent_params': agent_params,\n",
    "    'learning_rate': 0.5,\n",
    "    'discount_factor': 1,\n",
    "    \n",
    "}\n",
    "\n",
    "Sarsa_RL = Sarsa(Agent=Bob, Environment=Env, params=params)\n",
    "Q_RL = QLearning(Agent=Bob, Environment=Env, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS =1000\n",
    "Sarsa_RL.learn(N_EPOCHS, init_state=init_state)\n",
    "Q_RL.learn(N_EPOCHS, init_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  .   .   .   .   .   .   .   . -1. -1.]\n",
      " [  .   .   .   .   .   . -1. -1. -1.   .]\n",
      " [  .   .   .   . -1. -1. -1.   .   .   .]\n",
      " [  .   .   .   . -1.   .   .   .   .   .]\n",
      " [  .   .   .   . -1.   .   .   .   .   .]\n",
      " [  .   .   .   . -1.   .   .   .   .   .]\n",
      " [  .   .   .   . -1.   .   .   .   .   .]\n",
      " [  .   .   .   . -1.   .   .   .   .   .]\n",
      " [  .   .   .   . -1.   .   .   .   .   .]\n",
      " [ 0. -1. -1. -1. -1.   .   .   .   .   .]]\n",
      "Total reward : -18\n",
      "[[  .   .   .   .   .   .   .   .   . -1.]\n",
      " [  .   .   .   .   .   .   .   .   . -1.]\n",
      " [  .   .   .   .   .   .   .   .   . -1.]\n",
      " [  .   .   .   . -1. -1. -1. -1. -1. -1.]\n",
      " [  .   .   . -1. -1.   .   .   .   .   .]\n",
      " [  .   .   . -1.   .   .   .   .   .   .]\n",
      " [  .   .   . -1.   .   .   .   .   .   .]\n",
      " [  . -1. -1. -1.   .   .   .   .   .   .]\n",
      " [  . -1.   .   .   .   .   .   .   .   .]\n",
      " [ 0. -1.   .   .   .   .   .   .   .   .]]\n",
      "Total reward : -18\n"
     ]
    }
   ],
   "source": [
    "optimal_path, reward_history = Sarsa_RL.optimalPath(init_state)\n",
    "Env.printPath(optimal_path, reward_history)\n",
    "\n",
    "optimal_path, reward_history = Q_RL.optimalPath(init_state)\n",
    "Env.printPath(optimal_path, reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windy Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h, n_v = 10, 10\n",
    "terminal_state = (n_h-1, n_v-1)\n",
    "Env = WindyGridworld(n_h, n_v, terminal_state=terminal_state)\n",
    "#Env.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_params = {\n",
    "    'policy_name': 'EpsilonGreedy',\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    'state': init_state\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'policy_params': policy_params,\n",
    "    'agent_params': agent_params,\n",
    "    'learning_rate': 0.5,\n",
    "    'discount_factor': 1,\n",
    "    \n",
    "}\n",
    "\n",
    "Sarsa_RL = Sarsa(Agent=Bob, Environment=Env, params=params)\n",
    "Q_RL = QLearning(Agent=Bob, Environment=Env, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=1000\n",
    "Sarsa_RL.learn(N_EPOCHS, init_state=init_state)\n",
    "Q_RL.learn(N_EPOCHS, init_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  .   .   .   .   . -1. -1. -1. -1. -1.]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [ 0. -1. -1. -1. -1. -1.   .   .   .   .]]\n",
      "Total reward : -14\n",
      "[[  .   .   .   .   .   . -1. -1. -1. -1.]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   . -1.   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [ 0. -1. -1. -1. -1. -1.   .   .   .   .]]\n",
      "Total reward : -13\n"
     ]
    }
   ],
   "source": [
    "optimal_path, reward_history = Sarsa_RL.optimalPath(init_state)\n",
    "Env.printPath(optimal_path, reward_history)\n",
    "\n",
    "optimal_path, reward_history = Q_RL.optimalPath(init_state)\n",
    "Env.printPath(optimal_path, reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliff Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '*' '*' '*' '*' '*' '*' '*' '*' '.']]\n"
     ]
    }
   ],
   "source": [
    "n_h, n_v = 10, 10\n",
    "startover_state = (0, 0)\n",
    "terminal_state = (n_h-1, 0)\n",
    "Env = CliffGridworld(n_h, n_v, terminal_state=terminal_state, startover_state=startover_state)\n",
    "#Env.summary()\n",
    "Env.printCliff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_params = {\n",
    "    'policy_name': 'EpsilonGreedy',\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    'state': init_state\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'policy_params': policy_params,\n",
    "    'agent_params': agent_params,\n",
    "    'learning_rate': 0.5,\n",
    "    'discount_factor': 1,\n",
    "    \n",
    "}\n",
    "\n",
    "Sarsa_RL = Sarsa(Agent=Bob, Environment=Env, params=params)\n",
    "Q_RL = QLearning(Agent=Bob, Environment=Env, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=1000\n",
    "Sarsa_RL.learn(N_EPOCHS, init_state=init_state)\n",
    "Q_RL.learn(N_EPOCHS, init_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how Sarsa follows a safe path, away from the cliff, whereas Q-learning finds the optimal path, albeit more risky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1.   .   .   .   .   .   .   .   . -1.]\n",
      " [-1.   .   .   .   .   .   .   .   . -1.]\n",
      " [ 0.   .   .   .   .   .   .   .   . -1.]]\n",
      "Total reward : -15\n",
      "[[  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [  .   .   .   .   .   .   .   .   .   .]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [ 0.   .   .   .   .   .   .   .   . -1.]]\n",
      "Total reward : -11\n"
     ]
    }
   ],
   "source": [
    "optimal_path, reward_history = Sarsa_RL.optimalPath(init_state)\n",
    "Env.printPath(optimal_path, reward_history)\n",
    "\n",
    "optimal_path, reward_history = Q_RL.optimalPath(init_state)\n",
    "Env.printPath(optimal_path, reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
